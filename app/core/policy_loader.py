import os
import json
import re
import yaml
from pathlib import Path
from string import Template

from app.core.policy_engine import evaluate_policy
from app.models.policy_model import AgentResponse
from app.core.llm_connector import call_llm
from app.services.brand_resolver import resolve_brand_name

# üîß Racine du projet
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))


def load_yaml(path: str | Path) -> dict:
    path = Path(path)
    if not path.exists():
        raise FileNotFoundError(f"Policy file not found at: {path}")
    with path.open("r", encoding="utf-8") as file:
        return yaml.safe_load(file)


def interpolate_template(template_str: str, context: dict) -> str:
    try:
        return Template(template_str).safe_substitute(flatten_for_template(context))
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur interpolation : {e}")
        return template_str


def flatten_for_template(data: dict, parent_key='', sep='.') -> dict:
    items = {}
    for k, v in data.items():
        new_key = f"{parent_key}{sep}{k}" if parent_key else k
        if isinstance(v, dict):
            items.update(flatten_for_template(v, new_key, sep=sep))
        else:
            items[new_key] = v
    return items


async def load_policy_and_resolve(agent_name: str, user_context: dict) -> AgentResponse:
    # 1. Charger le fichier YAML
    policy_path = os.path.join(BASE_DIR, "static", "agents", "templates", f"{agent_name}.yml")
    policy_data = load_yaml(policy_path)

    # 2. √âvaluer les r√®gles YAML (if / elif / else)
    evaluated_response = await evaluate_policy(policy_data, user_context)

    # 3. R√©solution via LLM si demand√©
    if "message" in evaluated_response:
        raw_message = evaluated_response["message"]

        if "${llm_response}" in raw_message and "prompt_template" in policy_data:
            prompt = interpolate_template(policy_data["prompt_template"], user_context)
            llm_response = await call_llm(prompt=prompt, context=user_context, metadata=policy_data.get("metadata", {}))
            raw_message = raw_message.replace("${llm_response}", llm_response)

        evaluated_response["message"] = interpolate_template(raw_message, user_context)

    elif "prompt_template" in policy_data:
        prompt = interpolate_template(policy_data["prompt_template"], user_context)
        evaluated_response["message"] = await call_llm(prompt=prompt, context=user_context, metadata=policy_data.get("metadata", {}))

    else:
        evaluated_response["message"] = "ü§ñ I‚Äôm here, but no rule matched and no AI fallback was defined."

    # 4. Extraction √©ventuelle d‚Äôun bloc JSON structur√©
    context_update = {}

    try:
        match = re.search(r"```json\s*({[\s\S]+})\s*```", evaluated_response["message"])
        if match:
            context_update = json.loads(match.group(1))
            evaluated_response["message"] = evaluated_response["message"].split("```")[0].strip()
    except Exception as e:
        print("[‚ö†Ô∏è JSON extraction failed]", str(e))

    # 5. üîç V√©rification intelligente de la marque si first_piano.brand pr√©sent
    first_piano = context_update.get("first_piano", {})
    brand = first_piano.get("brand")

    if brand:
        email = user_context.get("email", "")
        result = await resolve_brand_name(brand, email)
        context_update["brand_resolution"] = result

        corrected = (
            result.get("matched_name") or
            result.get("corrected") or
            result.get("llm_data", {}).get("brand")
        )
        
        # ‚úÖ 5.a Cas : rejet√© ‚Üí on override le message directement
        if result["status"] == "rejected":
            evaluated_response["message"] = (
                f"‚ö†Ô∏è The brand **{brand}** doesn‚Äôt appear to be a known piano manufacturer.\n"
                f"If you're unsure, please upload a photo of the piano‚Äôs logo or fallboard."
            )
            evaluated_response["actions"] = [
                {
                    "label": "üì∏ Upload a photo",
                    "type": "upload",
                    "target": "photo_upload"
                }
            ]
             # üßπ Supprimer la marque du contexte pour √©viter l‚Äôaffichage dans le tableau
            context_update["first_piano"]["brand"] = ""

        # ‚úÖ 5.b Cas : correction ‚Üí injecte suggestion
        elif corrected and corrected != brand:
            context_update["first_piano"]["brand"] = corrected
            confirmation_msg = (
                f"\n\nüîé Did you mean **{corrected}** instead of **{brand}**? "
                "If you're unsure, feel free to upload a photo of your piano's logo or fallboard."
            )
            evaluated_response["message"] += confirmation_msg

    # 6. Retour
    return AgentResponse(
        message=evaluated_response.get("message", "No message"),
        actions=evaluated_response.get("actions", []),
        meta=evaluated_response.get("meta", {}),
        context_update=context_update or None
    )
